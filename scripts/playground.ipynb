{
 "cells": [
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 2,
=======
   "execution_count": 4,
>>>>>>> notebooks
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
<<<<<<< HEAD
      "15:50:35 rdkit INFO: Enabling RDKit 2022.09.5 jupyter extensions\n",
      "15:51:02 faiss.loader INFO: Loading faiss with AVX2 support.\n",
      "15:51:02 faiss.loader INFO: Successfully loaded faiss with AVX2 support.\n"
=======
      "10:20:41 rdkit INFO: Enabling RDKit 2022.09.5 jupyter extensions\n"
>>>>>>> notebooks
     ]
    }
   ],
   "source": [
<<<<<<< HEAD
    "from synnet.utils.reconstruct_utils import *"
=======
    "import os\n",
    "import pickle\n",
    "hash_dir = '/ssd/msun415/program_cache-bb=1000-prods=2/'\n",
    "programs = pickle.load(open(os.path.join(hash_dir, '2.pkl'), 'rb'))"
>>>>>>> notebooks
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/msun415/SynTreeNet/')\n",
    "from argparse import Namespace\n",
    "\n",
    "class Namespace:\n",
    "    def __init__(self, **kwargs):\n",
    "        self.__dict__.update(kwargs)\n",
    "\n",
    "args = Namespace(\n",
    "    rxns_collection_file='data/assets/reaction-templates/reactions_hb.json.gz',\n",
    "    rxn_templates_file='data/assets/reaction-templates/hb.txt',\n",
    "    building_blocks_file='data/assets/building-blocks/enamine_us_matched.csv',\n",
    "    embeddings_knn_file='data/assets/building-blocks/enamine_us_emb_fp_256.npy',\n",
    ")\n",
    "load_data(args)"
=======
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "from synnet.utils.data_utils import binary_tree_to_skeleton, Program\n",
    "\n",
    "def prog_to_binary_tree(prog):\n",
    "    g = nx.DiGraph()\n",
    "    i = 0\n",
    "    child_dic = {}\n",
    "    for e in prog.entries:\n",
    "        if isinstance(e, tuple):\n",
    "            g.add_node(i)\n",
    "            child_dic[e[0]] = child_dic.get(e[0], []) + [(i, e[1]==0)]\n",
    "            i += 1\n",
    "        else:\n",
    "            g.add_node(i)\n",
    "            g.add_node(i+1)            \n",
    "            child_dic[e] = child_dic.get(e, []) + [(i, False), (i+1, True)]\n",
    "            i += 2\n",
    "    r_dic = {}    \n",
    "    for r in prog.rxn_tree:\n",
    "        n = len(g)\n",
    "        g.add_node(n)\n",
    "        for c in list(prog.rxn_tree[r]):\n",
    "            interm = r_dic[c]\n",
    "            left = prog.rxn_tree.nodes[c]['child'] == 'left'\n",
    "            g.add_edge(n, interm, left=left)\n",
    "            g.nodes[interm]['left'] = left\n",
    "        childs = child_dic[r]\n",
    "        for c, left in childs:    \n",
    "            g.add_edge(n, c, left=left)\n",
    "            g.nodes[c]['left'] = left\n",
    "        r_dic[r] = n\n",
    "    return g\n",
    "    "
>>>>>>> notebooks
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synnet.utils.reconstruct_utils import rxns\n",
    "from synnet.utils.data_utils import Reaction, fp_2048\n",
    "from synnet.encoding.distances import _tanimoto_similarity"
=======
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "       \n",
    "\n"
>>>>>>> notebooks
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "skeletons = pickle.load(open('results/viz/top_1000/skeletons-top-1000.pkl', 'rb'))\n"
=======
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import heapq\n",
    "import multiprocessing as mp\n",
    "from multiprocessing.pool import ThreadPool\n",
    "from tqdm import tqdm\n",
    "from tdc import Oracle\n",
    "\n",
    "max_size = 128\n",
    "batch_size = 10000\n",
    "\n",
    "heap = []\n",
    "\n",
    "\n",
    "# def scoring(prog):\n",
    "#     oracle = Oracle(name=\"QED\")     \n",
    "#     prog.product_map.load()\n",
    "#     score = 0.\n",
    "#     prod_map = prog.product_map._product_map\n",
    "#     e = 1 if d == 2 else 0\n",
    "#     scores = [oracle(prod) for prod in list(prod_map[e])]\n",
    "#     score = max(scores)\n",
    "#     prog.product_map.unload()\n",
    "#     print(score)\n",
    "#     return score\n",
    "\n",
    "\n",
    "def score_single(smi, d, i):\n",
    "    oracle = Oracle(name=\"QED\")\n",
    "    res = []\n",
    "    if isinstance(smi, list):\n",
    "        for s in smi:\n",
    "            score = oracle(smi)\n",
    "            res.append((score, d, i))\n",
    "    else:\n",
    "        score = oracle(smi)\n",
    "        return (score, d, i)\n",
    "    return res\n",
    "\n",
    "def score_list(lis):\n",
    "    assert isinstance(lis, list)\n",
    "    oracle = Oracle(name=\"QED\")\n",
    "    res = []\n",
    "    for smi, d, i in lis:\n",
    "        score = oracle(smi)\n",
    "        res.append((score, d, i))\n",
    "    return res\n",
    "    \n",
    "    \n"
>>>>>>> notebooks
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 48\u001b[0m\n\u001b[1;32m     45\u001b[0m                         \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m bad_rxn_ids\n\u001b[0;32m---> 48\u001b[0m \u001b[43msanity_check\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m, in \u001b[0;36msanity_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m                 bad_rxn_ids\u001b[38;5;241m.\u001b[39madd(rxn_id)\n\u001b[1;32m     44\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m iou \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m                     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bad_rxn_ids\n",
      "Cell \u001b[0;32mIn[6], line 45\u001b[0m, in \u001b[0;36msanity_check\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m                 bad_rxn_ids\u001b[38;5;241m.\u001b[39madd(rxn_id)\n\u001b[1;32m     44\u001b[0m                 \u001b[38;5;28;01mif\u001b[39;00m iou \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.\u001b[39m:\n\u001b[0;32m---> 45\u001b[0m                     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m bad_rxn_ids\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1457\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.SafeCallWrapper.__call__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:701\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1152\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:1135\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.trace_dispatch\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m_pydevd_bundle/pydevd_cython.pyx:312\u001b[0m, in \u001b[0;36m_pydevd_bundle.pydevd_cython.PyDBFrame.do_wait_suspend\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m~/miniforge3/envs/synnet/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2070\u001b[0m, in \u001b[0;36mPyDB.do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, exception_type)\u001b[0m\n\u001b[1;32m   2067\u001b[0m             from_this_thread\u001b[38;5;241m.\u001b[39mappend(frame_custom_thread_id)\n\u001b[1;32m   2069\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_threads_suspended_single_notification\u001b[38;5;241m.\u001b[39mnotify_thread_suspended(thread_id, thread, stop_reason):\n\u001b[0;32m-> 2070\u001b[0m         keep_suspended \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_do_wait_suspend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mevent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43marg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msuspend_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfrom_this_thread\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframes_tracker\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2072\u001b[0m frames_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   2074\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keep_suspended:\n\u001b[1;32m   2075\u001b[0m     \u001b[38;5;66;03m# This means that we should pause again after a set next statement.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniforge3/envs/synnet/lib/python3.9/site-packages/debugpy/_vendored/pydevd/pydevd.py:2106\u001b[0m, in \u001b[0;36mPyDB._do_wait_suspend\u001b[0;34m(self, thread, frame, event, arg, suspend_type, from_this_thread, frames_tracker)\u001b[0m\n\u001b[1;32m   2103\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_input_hook()\n\u001b[1;32m   2105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprocess_internal_commands()\n\u001b[0;32m-> 2106\u001b[0m     \u001b[43mtime\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2108\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcancel_async_evaluation(get_current_thread_id(thread), \u001b[38;5;28mstr\u001b[39m(\u001b[38;5;28mid\u001b[39m(frame)))\n\u001b[1;32m   2110\u001b[0m \u001b[38;5;66;03m# process any stepping instructions\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def sanity_check():\n",
    "    bad_rxn_ids = set()\n",
    "    for index in range(len(skeletons)):\n",
    "        key = list(skeletons)[index]        \n",
    "        for st in skeletons[key]:\n",
    "            sk = Skeleton(st, index)\n",
    "            for n in np.argwhere(~sk.rxns & ~sk.leaves).flatten():             \n",
    "                r = list(sk.tree.successors(n))[0]\n",
    "                rxn_id = sk.tree.nodes[r]['rxn_id']\n",
    "                rxn = rxns[rxn_id]\n",
    "                smirks = rxn.smirks\n",
    "                smiles = sk.tree.nodes[sk.tree_root]['smiles']\n",
    "                retro_smirks = '>>'.join(smirks.split('>>')[::-1])\n",
    "                retro_rxn = Reaction(retro_smirks)\n",
    "                if retro_rxn.num_reactant == 2:\n",
    "                    print(\"hi\")\n",
    "                all_prods = retro_rxn.rxn.RunReactants((retro_rxn.get_mol(smiles),))      \n",
    "                succs = list(sk.tree.successors(r))\n",
    "                if sk.tree.nodes[succs[0]]['child'] == 'right':\n",
    "                    succs = succs[::-1]\n",
    "                precursors = [sk.tree.nodes[n]['smiles'] for n in succs]\n",
    "                iou = 0.\n",
    "                for prods in all_prods:\n",
    "       \n",
    "                    # if len(prods) == 2 and not rxn.is_reactant_first(prods[0]):\n",
    "                    #     assert rxn.is_reactant_second(prods[0])\n",
    "                    #     assert rxn.is_reactant_first(prods[1])\n",
    "                    #     prods = prods[::-1]\n",
    "                    # else:\n",
    "                    #     assert rxn.is_reactant_first(prods[0])\n",
    "                    #     if len(prods) > 1:\n",
    "                    #         assert rxn.is_reactant_second(prods[1])\n",
    "    \n",
    "                    smis = [Chem.MolToSmiles(p) for p in prods]\n",
    "                    try:\n",
    "                        prod_fps = [np.array(fp_2048(smi)) for smi in smis]\n",
    "                        pre_fps = [np.array(fp_2048(smi)) for smi in precursors]\n",
    "                    except:\n",
    "                        continue\n",
    "                    ious = [_tanimoto_similarity(fp1, fp2) for (fp1, fp2) in zip(prod_fps, pre_fps)]\n",
    "                    iou = max(iou, np.mean(ious))\n",
    "                if iou < 1:\n",
    "                    bad_rxn_ids.add(rxn_id)\n",
    "                    if iou > 0.:\n",
    "                        pass\n",
    "    return bad_rxn_ids\n",
    "\n",
    "sanity_check()\n",
    "\n",
    "\n"
=======
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.seed(0)\n",
    "batches_per_program = 1\n",
    "fpath = '/home/msun415/SynTreeNet/scores-qed.txt'\n",
    "heap = []\n",
    "max_size = 128\n",
    "\n",
    "def load_file(fpath):\n",
    "    res = []\n",
    "    with open(fpath) as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        for line in tqdm(lines, desc=\"processing lines\"):\n",
    "            line = line.strip('(').rstrip(')\\n')\n",
    "            if not line:\n",
    "                continue  \n",
    "            res.append(line)\n",
    "        return res\n",
    "    \n",
    "# lines = load_file(fpath)\n",
    "\n",
    "def re_eval():\n",
    "    for line in lines:\n",
    "        score, d, ind = line.split(', ')\n",
    "        print(line)\n",
    "        d = int(d)\n",
    "        i = int(ind)\n",
    "        prog = programs[d][int(ind)]\n",
    "        prog.product_map.load()\n",
    "        prod_map = prog.product_map._product_map\n",
    "        e = 1 if d == 2 else 0\n",
    "        smis = [(prod, d, i) for prod in list(prod_map[e])]\n",
    "        random.shuffle(smis)\n",
    "        smis = smis[:batches_per_program*batch_size] # only do first batch\n",
    "        inner_batch_size = (len(smis)+99)//100\n",
    "        smi_lists = [smis[i*inner_batch_size:(i+1)*inner_batch_size] for i in range(100)]\n",
    "        with mp.Pool(100) as p:\n",
    "            scores = p.map(score_list, tqdm(smi_lists, desc=\"single batch\"))        \n",
    "        res = [r for res_lis in scores for r in res_lis]\n",
    "        for (score, d, j), smi in zip(res, smis):\n",
    "            if len(heap) < max_size:\n",
    "                heapq.heappush(heap, (score, d, j, smi))\n",
    "            else:\n",
    "                small = heapq.heappushpop(heap, (score, d, j, smi))\n",
    "        print(heap)\n",
    "\n",
    "\n",
    "fpath = '/home/msun415/SynTreeNet/scores-logp.txt'\n",
    "def load_file2(fpath):\n",
    "    res = []\n",
    "    with open(fpath) as f:\n",
    "        lines = f.readlines()[1:]\n",
    "        for line in tqdm(lines, desc=\"processing lines\"):\n",
    "            line = line.strip('(').rstrip(')\\n')\n",
    "            if not line:\n",
    "                continue  \n",
    "            res.append(line)\n",
    "        return res"
>>>>>>> notebooks
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[fp_2048(precursor) for precursor in precursors]"
=======
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from synnet.encoding.fingerprints import fp_2048\n",
    "import json\n",
    "\n",
    "def lines_to_json(heap):\n",
    "    dics = []\n",
    "    for score, d, j, smi in heap:\n",
    "        dic = {}\n",
    "        d = int(d)\n",
    "        j = int(j)\n",
    "        tree = prog_to_binary_tree(programs[d][j])\n",
    "        root = next(v for v, d in tree.in_degree() if d == 0)\n",
    "        dic['bt'] = nx.tree_data(tree, root)\n",
    "        dic['fp'] = fp_2048(smi)\n",
    "        dic['smi'] = smi\n",
    "        dics.append(dic)\n",
    "    return dics\n",
    "\n",
    "# json.dump(lines_to_json(heap), open('/home/msun415/SynTreeNet/indvs-qed.json', 'w+'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing lines: 100%|██████████| 131/131 [00:00<00:00, 906689.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msun415/SynTreeNet/indvs-drd2.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing lines: 100%|██████████| 131/131 [00:00<00:00, 1445931.12it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msun415/SynTreeNet/indvs-jnk.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "processing lines: 100%|██████████| 131/131 [00:00<00:00, 1489034.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/msun415/SynTreeNet/indvs-logp.json\n"
     ]
    }
   ],
   "source": [
    "for prop in ['drd2','jnk','logp']:\n",
    "    heap = [l.split(', ') for l in load_file2(f'/home/msun415/SynTreeNet/scores-{prop}.txt')]\n",
    "    json.dump(lines_to_json(heap), open(f'/home/msun415/SynTreeNet/indvs-{prop}.json', 'w+'))\n",
    "    print(f'/home/msun415/SynTreeNet/indvs-{prop}.json')"
>>>>>>> notebooks
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
<<<<<<< HEAD
   "source": [
    "Pdb.set_trace()"
   ]
=======
   "source": []
>>>>>>> notebooks
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "synnet",
   "language": "python",
<<<<<<< HEAD
   "name": "synnet"
=======
   "name": "python3"
>>>>>>> notebooks
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
<<<<<<< HEAD
   "version": "3.9.19"
=======
   "version": "3.9.18"
>>>>>>> notebooks
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
